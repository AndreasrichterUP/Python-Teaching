{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics of Machine Learning\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview of the notebook:**\n",
    "    - Perceptrons\n",
    "    - Regression\n",
    "    - Classification\n",
    "    - Cross-Validation\n",
    "    - Visualze Performance\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptrons**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very basic beginning of machine learning algorithms was founded by Frank Rosenbaum in the 1950s. He set up a visual/mechanical hardware that was able to separate pictures of...\n",
    "\n",
    "Rosenbaum trained the machine using example datasets (pictures) and offering the right decisions to each datapoint. Than letting the automate decide about unseen datasets to measure the performance....\n",
    "\n",
    "The very basic rule to decide was the zero-one loss ....\n",
    "\n",
    "A very common function to map a random real number to the classes [0,1] is the sigmoid-function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d67bb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "x=np.arange(-7.5,7.5,0.2)\n",
    "#print(x)\n",
    "y=1/(1+np.exp(-x))\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmiod is defined as the $y(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "It enabales a the setup of a decision rule like $ y(x) < 0.5 \\Rightarrow $ 0 and \n",
    " $ y(x) \\ge 0.5 \\Rightarrow $ 1.\n",
    " \n",
    "The follwing little function calculates the results of the sigmoid for every real number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(a):\n",
    "    b=1/(1+math.exp(-a) )\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of $0$ will result in a sigmoid(0)=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any positive number will result in a sigmoid of greater 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.549833997312478\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and any negative number will result in a sigmoid of lower than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45016600268752216\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(-0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater or smaller the numbers are the more clear the output of the sigmoid will converge to 1 or to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999979388463\n",
      "2.0611536181902037e-09\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(20))\n",
    "print(sigmoid(-20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the sigmoid enables a decision finding interpreted as a probability number between 0 and 1 and a classification rule of lower or higher than 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of regression the model tries to predict a floating point value of a continous variable. Regression is a tool to **deduct** from a sampled dataset to the general **circumstances** in the whole data. In other words I want to know which value I can expect at the positions where I wasn't able to measure a datapoint. \n",
    "\n",
    "Calculation models used in this case provide often exact mathematical solution without any optimizational help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of classification the model tries to predict a nominal value, a class number a true/false decision or category in a taxometric system. \n",
    "\n",
    "Calculation model used in this case are often not able to find local/global minima by direct or exact mathematical aproaches and need the help of optimization techniques like gradient decent, stochastic gradient decent or others. Classifictions are more computationally expensiv than regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the name Logistic regression is a classification method. It was first introduced and named in statistics and later on used in an other context by the machine learning community. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "[[ 1.  ]\n",
      " [ 0.2 ]\n",
      " [ 0.5 ]\n",
      " [ 0.91]]\n"
     ]
    }
   ],
   "source": [
    "xa=np.zeros((4,1))\n",
    "print(xa)\n",
    "xa[0,0]= 1\n",
    "xa[1,0]= 0.2\n",
    "xa[2,0]= 0.5\n",
    "xa[3,0]= 0.91\n",
    "print(xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "[[ 1.  ]\n",
      " [ 0.4 ]\n",
      " [ 0.01]\n",
      " [ 0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "xb=np.zeros((4,1))\n",
    "print(xb)\n",
    "xb[0,0]= 1\n",
    "xb[1,0]= 0.4\n",
    "xb[2,0]= 0.01\n",
    "xb[3,0]= 0.5\n",
    "\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "[[ 1. ]\n",
      " [ 0.3]\n",
      " [ 1.1]\n",
      " [ 0.8]]\n"
     ]
    }
   ],
   "source": [
    "xc=np.zeros((4,1))\n",
    "print(xc)\n",
    "xc[0,0]= 1\n",
    "xc[1,0]= 0.3\n",
    "xc[2,0]= 1.1\n",
    "xc[3,0]= 0.8\n",
    "print(xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "[[ 0.66]\n",
      " [ 0.1 ]\n",
      " [ 0.35]\n",
      " [ 0.7 ]]\n"
     ]
    }
   ],
   "source": [
    "w=np.zeros((4,1))\n",
    "print(w)\n",
    "w[0,0]= 0.66\n",
    "w[1,0]= 0.1\n",
    "w[2,0]= 0.35\n",
    "w[3,0]= 0.7\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8163782718851771"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(xa.T,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the label was \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7414464286583212"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(xb.T,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the label was \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8368534378594309"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(xc.T,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the label was \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the sum of the squared error by \n",
    "\n",
    "$\\frac{1}{2}[ (1 - 0.8163 )^2  + (0 - 0.7414)^2 +  (0 - 0.8368 )^2 ]$\n",
    "\n",
    "we get a quite bad result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6418269449999999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/2* ((1 - 0.8163 )**2  + (0 - 0.7414)**2 +  (0 - 0.8368 )**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==============================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now update the weights vector $\\vec w$ e.g. to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "[[ 0.25]\n",
      " [ 0.1 ]\n",
      " [ 0.36]\n",
      " [ 0.3 ]]\n"
     ]
    }
   ],
   "source": [
    "w=np.zeros((4,1))\n",
    "print(w)\n",
    "w[0,0]= 0.25\n",
    "w[1,0]= 0.1\n",
    "w[2,0]= 0.36\n",
    "w[3,0]= 0.3\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def np_sigmoid(a):\n",
    "    b=1/(1+np.exp(-a) )\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673267295119654"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(xa.T,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6091165035715496"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(xb.T,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142263775539306"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(xc.T,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and recalculate the squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4939413449999999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/2* ((1 - 0.6732 )**2  + (0 - 0.6091)**2 +  (0 - 0.7142 )**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.    1.    1.  ]\n",
      " [ 0.2   0.4   0.3 ]\n",
      " [ 0.5   0.01  1.1 ]\n",
      " [ 0.91  0.5   0.8 ]]\n"
     ]
    }
   ],
   "source": [
    "x=np.concatenate((xa,xb,xc),axis=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.723 ]\n",
      " [ 0.4436]\n",
      " [ 0.916 ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(x.T,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6732673 ]\n",
      " [ 0.6091165 ]\n",
      " [ 0.71422638]]\n"
     ]
    }
   ],
   "source": [
    "print(np_sigmoid(np.dot(x.T,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique to estimate the goodness-of-fit of a model to the given data or in other words how good the model will generalize on an independent dataset. It is mainly used in settings were the accuracy of a predictive model has to be estimated. Therefore the dataset is divided into k random subsets for k experiments using each time k-1 subsets to train k models which are tested on the respectively remaining subset. Figure 11 shows this approach for k=5 subsets/experiments. The testing errors are averaged which provides an estimate on how good the model generalizes on the whole dataset. This technique is called k-Fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC Cuves and Tuckey Box plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of unbalanced classes, e.g. only few positive and a lot of negative data-points the classification accuracy is an insuffi- cient performance measure as a classifier that would predict the negative class for the whole dataset would earn a very high accu- racy without any correct prediction of the positive class. Receiver Operator Characteristics (ROC- curves) serve as basis for an in- dependent measure of performance (ROCAUC) which is widely used for such cases in machine learning. The ROC-curve shows a diagram with the false-positive rate ( P(FP), specificity or FPR, horizontal axis) vs. the true-positive rate ( P(TP), sensitivity or TPR, vertical axis) of a classifier. Both relative frequencies speci- ficity & sensitivity are evaluated at any cutpoint between the two class distributions and displayed in a diagram which represents the trade-off between them. (cf. figure 13) For practical usage a two-digit number of cutpoints is sufficient to approximate the ROC-curve. The interpretation of the curve is easy comprehensi- ble if for example the discrimination of the used classifier is very good the false-positive rate is very low while the true-positive rate is relative high. This results in a very steep curve which also means that the area under the curve is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
